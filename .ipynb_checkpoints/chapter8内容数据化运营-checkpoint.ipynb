{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于潜在狄里克雷分配（LDA）的内容主题挖掘 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./TF-IDF.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T00:56:25.965631Z",
     "start_time": "2019-03-25T00:56:25.301946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./news_data/news.sohunews.010806.txt', './news_data/news.sohunews.020806.txt', './news_data/news.sohunews.030806.txt', './news_data/news.sohunews.040806.txt', './news_data/news.sohunews.050806.txt', './news_data/news.sohunews.060806.txt', './news_data/news.sohunews.070806.txt', './news_data/news.sohunews.080806.txt', './news_data/news.sohunews.110806.txt', './news_data/news.sohunews.120806.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile  # tar压缩包库\n",
    "if not os.path.exists('./news_data'):\n",
    "    with tarfile.open('./datacode_for_book/chapter8/news_data.tar.gz') as tar:\n",
    "        print(tar.getnames())\n",
    "        for name in tar.getmembers():\n",
    "            tar.extract(name,path='./datacode_for_book/chapter8/')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T00:56:26.028138Z",
     "start_time": "2019-03-25T00:56:25.967082Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 全角转半角\n",
    "def str_convert(content):\n",
    "    '''\n",
    "    将内容中的全角字符，包含英文字母、数字键、符号等转换为半角字符\n",
    "    :param content: 要转换的字符串内容\n",
    "    :return: 转换后的半角字符串\n",
    "    '''\n",
    "    new_str = ''\n",
    "    for each_char in content:  # 循环读取每个字符\n",
    "        code_num = ord(each_char)  # 读取字符的ASCII值或Unicode值\n",
    "        if code_num == 12288:  # 全角空格直接转换\n",
    "            code_num = 32\n",
    "        elif (code_num >= 65281 and code_num <= 65374):  # 全角字符（除空格）根据关系转化\n",
    "            code_num -= 65248\n",
    "        new_str += chr(code_num)\n",
    "    return new_str\n",
    "\n",
    "def data_parse(data):\n",
    "    '''\n",
    "    从原始文件中解析出文本内容数据\n",
    "    :param data: 包含代码的原始内容\n",
    "    :return: 文本中的所有内容，列表型\n",
    "    '''\n",
    "    raw_code = BeautifulSoup(data, \"lxml\")  # 建立BeautifulSoup对象\n",
    "    content_code = raw_code.find_all('content')  # 从包含文本的代码块中找到content标签\n",
    "    content_list = []  # 建立空列表，用来存储每个content标签的内容\n",
    "    for each_content in content_code:  # 循环读出每个content标签\n",
    "        if len(each_content) > 0:  # 如果content标签的内容不为空\n",
    "            raw_content = each_content.text  # 获取原始内容字符串\n",
    "            convert_content = str_convert(raw_content)  # 将全角转换为半角\n",
    "            content_list.append(convert_content)  # 将content文本内容加入列表\n",
    "    return content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T00:56:35.433231Z",
     "start_time": "2019-03-25T00:56:26.029577Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk files and get content...\n"
     ]
    }
   ],
   "source": [
    "# 汇总所有内容\n",
    "import os\n",
    "print ('walk files and get content...')\n",
    "all_content = []  # 总列表，用于存储所有文件的文本内容\n",
    "for root, dirs, files in os.walk('./datacode_for_book/chapter8/news_data'):  # 分别读取遍历目录下的根目录、子目录和文件列表\n",
    "    for file in files:  # 读取每个文件\n",
    "        file_name = os.path.join(root, file)  # 将目录路径与文件名合并为带有完整路径的文件名\n",
    "        with open(file_name,'r',encoding='utf-8') as f:  # 以只读方式打开文件\n",
    "            data = f.read()  # 读取文件内容\n",
    "        all_content.extend(data_parse(data))  # 从文件内容中获取文本并将结果追加到总列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T00:56:36.281884Z",
     "start_time": "2019-03-25T00:56:35.435710Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "# 中文分词\n",
    "def jieba_cut(text):\n",
    "    '''\n",
    "    将输入的文本句子根据词性标注做分词\n",
    "    :param text: 文本句子，字符串型\n",
    "    :return: 符合规则的分词结果\n",
    "    '''\n",
    "    rule_words = ['z', 'vn', 'v', 't', 'nz', 'nr', 'ns', 'n', 'l', 'i', 'j', 'an',\n",
    "                  'a']  # 只保留状态词、名动词、动词、时间词、其他名词、人名、地名、名词、习用语、简称略语、成语、形容词、名形词\n",
    "    words = pseg.cut(text)  # 分词\n",
    "    seg_list = []  # 列表用于存储每个文件的分词结果\n",
    "    for word in words:  # 循环得到每个分词\n",
    "        if word.flag in rule_words:\n",
    "            seg_list.append(word.word)  # 将分词追加到列表\n",
    "    return seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:03:15.638439Z",
     "start_time": "2019-03-25T00:56:36.284366Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\longf\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get word list...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.778 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 获取每条内容的分词结果\n",
    "print ('get word list...')\n",
    "words_list = []  # 分词列表，用于存储所有文件的分词结果\n",
    "for each_content in all_content:  # 循环读出每个文本内容\n",
    "    words_list.append(list(jieba_cut(each_content)))  # 将文件内容的分词结果以列表的形式追加到列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:03:16.123774Z",
     "start_time": "2019-03-25T01:03:15.641369Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\ProgramData\\Anaconda3\\envs\\normal\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# 文本预处理\n",
    "from gensim import corpora, models  # gensim的词频统计和主题建模模块\n",
    "def text_pro(words_list, tfidf_object=None, training=True):\n",
    "    '''\n",
    "    gensim主题建模预处理过程，包含分词类别转字典、生成语料库和TF-IDF转换\n",
    "    :param words_list: 分词列表，列表型\n",
    "    :param tfidf_object: TF-IDF模型对象，该对象在训练阶段生成\n",
    "    :param training: 是否训练阶段，用来针对训练和预测两个阶段做预处理\n",
    "    :return: 如果是训练阶段，返回词典、TF-IDF对象和TF-IDF向量空间数据；如果是预测阶段，返回TF-IDF向量空间数据\n",
    "    '''\n",
    "    # 分词列表转字典\n",
    "    dic = corpora.Dictionary(words_list)  # 将分词列表转换为字典形式\n",
    "    print(('{:*^60}'.format('token & word mapping review:')))\n",
    "    for i, w in list(dic.items())[:5]:  # 循环读出字典前5条的每个key和value，对应的是索引值和分词\n",
    "        print(('token:%s -- word:%s' % (i, w)))\n",
    "    # 生成语料库\n",
    "    corpus = []  # 建立一个用于存储语料库的列表\n",
    "    for words in words_list:  # 读取每个分词列表\n",
    "        corpus.append(dic.doc2bow(words))  # 将每个分词列表转换为语料库词袋（bag of words）形式的列表\n",
    "    print(('{:*^60}'.format('bag of words review:')))\n",
    "    print((corpus[0]))  # 打印输出第一条语料库\n",
    "    # TF-IDF转换\n",
    "    if training == True:\n",
    "        tfidf = models.TfidfModel(corpus)  # 建立TF-IDF模型对象\n",
    "        corpus_tfidf = tfidf[corpus]  # 得到TF-IDF向量稀疏矩阵\n",
    "        print(('{:*^60}'.format('TF-IDF model review:')))\n",
    "        for doc in corpus_tfidf:  # 循环读出每个向量\n",
    "            print(doc)  # 打印第一条向量\n",
    "            break  # 跳出循环\n",
    "        return dic, corpus_tfidf, tfidf\n",
    "    else:\n",
    "        return tfidf_object[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:03:48.417932Z",
     "start_time": "2019-03-25T01:03:16.126457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train topic model...\n",
      "****************token & word mapping review:****************\n",
      "token:0 -- word:仇恨\n",
      "token:1 -- word:侮辱\n",
      "token:2 -- word:侵害\n",
      "token:3 -- word:凶杀\n",
      "token:4 -- word:危害\n",
      "********************bag of words review:********************\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 2), (17, 1), (18, 1), (19, 2), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1)]\n",
      "********************TF-IDF model review:********************\n",
      "[(0, 0.16762633852828174), (1, 0.16660204914253687), (2, 0.1643986382302142), (3, 0.168282481745965), (4, 0.16197667368712637), (5, 0.14602961468426073), (6, 0.16282320045073903), (7, 0.10154448591145282), (8, 0.12365275311464316), (9, 0.12399080729729553), (10, 0.16703117734810868), (11, 0.163124879458702), (12, 0.16844765669812112), (13, 0.16409043499326897), (14, 0.1662290891913951), (15, 0.1685028172752526), (16, 0.332245916102828), (17, 0.16383481532598135), (18, 0.16681622559479037), (19, 0.30849126342177313), (20, 0.1677351934753784), (21, 0.16778969587205647), (22, 0.15736459689355045), (23, 0.15266091940783724), (24, 0.11609101090194619), (25, 0.2636835311342954), (26, 0.14576561774317554), (27, 0.16762633852828174), (28, 0.16751768276692697), (29, 0.1653853043789113), (30, 0.16501988564410103), (31, 0.16833748902827933)]\n",
      "********************topic model review:*********************\n",
      "0.004*\"散布\" + 0.004*\"民族\" + 0.004*\"稳定\" + 0.004*\"登录\" + 0.003*\"标题\" + 0.002*\"犯罪\" + 0.002*\"谣言\" + 0.002*\"赌博\" + 0.002*\"教唆\" + 0.002*\"封建迷信\"\n",
      "0.002*\"比赛\" + 0.002*\"是\" + 0.001*\"搜狐\" + 0.001*\"中国\" + 0.001*\"有\" + 0.001*\"北京\" + 0.001*\"人\" + 0.001*\"说\" + 0.001*\"欧洲杯\" + 0.001*\"地震\"\n",
      "0.003*\"小区\" + 0.003*\"编号\" + 0.002*\"户型\" + 0.002*\"面积\" + 0.002*\"装修\" + 0.002*\"楼层\" + 0.002*\"建筑面积\" + 0.002*\"室\" + 0.002*\"厅\" + 0.002*\"有效期\"\n"
     ]
    }
   ],
   "source": [
    "# 建立主题模型\n",
    "print ('train topic model...')\n",
    "dic, corpus_tfidf, tfidf = text_pro(words_list, tfidf_object=None, training=True)  # 训练集的文本预处理\n",
    "num_topics = 3  # 设置主题个数\n",
    "lda = models.LdaModel(corpus_tfidf, id2word=dic, num_topics=num_topics)  # 通过LDA进行主题建模\n",
    "print ('{:*^60}'.format('topic model review:'))\n",
    "for i in range(num_topics):  # 输出每一类主题的结果\n",
    "    print (lda.print_topic(i))  # 输出对应主题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:03:49.511486Z",
     "start_time": "2019-03-25T01:03:48.419725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic forecast...\n",
      "****************token & word mapping review:****************\n",
      "token:0 -- word:一鸣惊人\n",
      "token:1 -- word:三剑客\n",
      "token:2 -- word:上演\n",
      "token:3 -- word:不败\n",
      "token:4 -- word:专业培训\n",
      "********************bag of words review:********************\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 3), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 2), (16, 2), (17, 1), (18, 1), (19, 3), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 2), (28, 3), (29, 2), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 2), (36, 1), (37, 2), (38, 1), (39, 1), (40, 2), (41, 2), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 2), (51, 1), (52, 1), (53, 1), (54, 2), (55, 3), (56, 1), (57, 1), (58, 1), (59, 2), (60, 1), (61, 1), (62, 2), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 2), (71, 1), (72, 1), (73, 4), (74, 1), (75, 1), (76, 1), (77, 7), (78, 5), (79, 2), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 4), (90, 4), (91, 1), (92, 1), (93, 7), (94, 1), (95, 1), (96, 2), (97, 3), (98, 1), (99, 2), (100, 2), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 8), (107, 1), (108, 3), (109, 1), (110, 1), (111, 3), (112, 2), (113, 1), (114, 1), (115, 1), (116, 2), (117, 1), (118, 1), (119, 1), (120, 7), (121, 2), (122, 4), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 17), (130, 1), (131, 4), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 2), (140, 2), (141, 2), (142, 1), (143, 1), (144, 1), (145, 2), (146, 1), (147, 2), (148, 1), (149, 2), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 4), (156, 1), (157, 1), (158, 2), (159, 2), (160, 1), (161, 1), (162, 5), (163, 1), (164, 2), (165, 1), (166, 6), (167, 1), (168, 1), (169, 1), (170, 1), (171, 2), (172, 1), (173, 1), (174, 1), (175, 1), (176, 2), (177, 1), (178, 1), (179, 3), (180, 1), (181, 1), (182, 1), (183, 3), (184, 1), (185, 1), (186, 2)]\n",
      "**********************topic forecast:***********************\n",
      "[[(0, 0.24114716), (1, 0.68917763), (2, 0.069675244)]]\n"
     ]
    }
   ],
   "source": [
    "# 新数据集的主题模型预测\n",
    "print ('topic forecast...')\n",
    "with open('./datacode_for_book/chapter8/article.txt','r',encoding='utf-8') as f:  # 打开新的文本\n",
    "    text_new = f.read()  # 读取文本数据\n",
    "text_content = data_parse(data)  # 解析新的文本\n",
    "words_list_new = jieba_cut(text_new)  # 将文本转换为分词列表\n",
    "corpus_tfidf_new = text_pro([words_list_new], tfidf_object=tfidf, training=False)  # 新文本数据集的预处理\n",
    "corpus_lda_new = lda[corpus_tfidf_new]  # 获取新的分词列表（文档）的主题概率分布\n",
    "print ('{:*^60}'.format('topic forecast:'))\n",
    "print (list(corpus_lda_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T00:08:12.007073Z",
     "start_time": "2019-03-27T00:08:08.950038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on 2017-12-12\\n\\ngensim API地址:\\nhttps://radimrehurek.com/gensim/apiref.html\\n\\n本篇对gensim讲解分为3大类\\n1.gensim字典的基本使用，其中和jieba结合使用\\n2.gensim模型的使用,比如tf-idf模型，lsi模型（用于求文本相似度）等\\n3.gensim的数据类型和Numpy，Scipy的转换\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\ProgramData\\Anaconda3\\envs\\normal\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\longf\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.731 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#coding=utf-8\n",
    "'''\n",
    "Created on 2017-12-12\n",
    "\n",
    "gensim API地址:\n",
    "https://radimrehurek.com/gensim/apiref.html\n",
    "\n",
    "本篇对gensim讲解分为3大类\n",
    "1.gensim字典的基本使用，其中和jieba结合使用\n",
    "2.gensim模型的使用,比如tf-idf模型，lsi模型（用于求文本相似度）等\n",
    "3.gensim的数据类型和Numpy，Scipy的转换\n",
    "'''\n",
    "import jieba\n",
    "from gensim import corpora, models, similarities\n",
    "# 一.gensim字典的基本使用\n",
    "# 加载数据\n",
    "# 从文件加载\n",
    "files = []\n",
    "f1 = open(\"./gensim1.txt\", \"r\",encoding='utf-8').read()\n",
    "f2 = open(\"./gensim2.txt\", \"r\",encoding='utf-8').read()\n",
    "files.append(f1)\n",
    "files.append(f2)\n",
    "\n",
    "# 文件数据切词\n",
    "text1 = [[word for word in jieba.cut(f1)]]\n",
    "text2 = [[word for word in jieba.cut(f2)]]\n",
    "texts = [[word for word in jieba.cut(file)] for file in files]\n",
    "\n",
    "# 生成字典,prune_at的作用为控制向量的维数，也就是说最多能为2000000个词语进行向量化\n",
    "# dictionary = corpora.Dictionary(texts,prune_at=2000000)\n",
    "# 可以这样分开添加使用\n",
    "dictionary = corpora.Dictionary(text1,prune_at=2000000)\n",
    "# # 对字典进行扩容\n",
    "dictionary.add_documents(text2, prune_at=2000000)\n",
    "# 或者手动设置一次词语，但这个方法一般用来打印所有的词语和编号print dictionary.token2id或者print dictionary.id2token\n",
    "# dictionary.token2id = {'computer': 0, 'human': 1, 'response': 2, 'survey': 3}\n",
    "# 合并字典\n",
    "# dict1 = corpora.Dictionary(some_documents)\n",
    "# dict2 = corpora.Dictionary(other_documents)\n",
    "# dict2_to_dict1 = dict1.merge_with(dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T00:08:12.039985Z",
     "start_time": "2019-03-27T00:08:12.016064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['在', '我', '玉龙雪山', '喜欢', '去', '还要']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{1: 3, 0: 1, 2: 3, 3: 1, 5: 1, 4: 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'还要'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据\n",
    "wordslist = [\"我在玉龙雪山\",\"我喜欢玉龙雪山\",\"我还要去玉龙雪山\"] \n",
    "# 切词\n",
    "textTest = [[word for word in jieba.cut(words)] for words in wordslist]\n",
    "# 生成字典\n",
    "dictionary = corpora.Dictionary(textTest,prune_at=2000000)\n",
    "# 保存生成的字典\n",
    "dictionary.save('./gensim_dict.dict')\n",
    "#load数据\n",
    "# dictionary.load('./gensim_dict.dict')\n",
    "\n",
    "# 遍历字典\n",
    "dictionary.keys()    #返回所有词语的编号\n",
    "list(dictionary.values())\n",
    "dictionary.dfs    #{单词id，在多少文档中出现}\n",
    "dictionary.get(5) #返回编号对应的词语，例如这里5->特性。\n",
    "dictionary.compactify() #压缩词语向量，如果使用了filter_extremes，filter_n_most_frequent，filter_tokens等删除词典元素的操作，可以使用该方法进行压缩\n",
    "dictionary.num_docs #所有文章数目\n",
    "dictionary.num_nnz #每个文件中不重复词个数的和\n",
    "dictionary.num_pos #所有词的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T00:08:12.056016Z",
     "start_time": "2019-03-27T00:08:12.049960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 在 1\n",
      "1 我 3\n",
      "2 玉龙雪山 3\n",
      "3 喜欢 1\n",
      "4 去 1\n",
      "5 还要 1\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "wordslist = [\"我在玉龙雪山\",\"我喜欢玉龙雪山\",\"我还要去玉龙雪山\"] \n",
    "# 切词\n",
    "textTest = [[word for word in jieba.cut(words)] for words in wordslist]\n",
    "# 生成字典\n",
    "dictionary = corpora.Dictionary(textTest,prune_at=2000000)\n",
    "for key in dictionary.keys():\n",
    "    print(key,dictionary.get(key),dictionary.dfs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T00:08:12.071714Z",
     "start_time": "2019-03-27T00:08:12.066725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 在 1\n",
      "1 喜欢 1\n",
      "2 去 1\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "wordslist = [\"我在玉龙雪山\",\"我喜欢玉龙雪山\",\"我还要去玉龙雪山\"] \n",
    "# 切词\n",
    "textTest = [[word for word in jieba.cut(words)] for words in wordslist]\n",
    "# 生成字典\n",
    "dictionary = corpora.Dictionary(textTest,prune_at=2000000)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.5,keep_n=3 ) #过滤文档频率大于no_below，小于no_above*num_docs（3）的词\n",
    "for key in dictionary.keys():\n",
    "    print(key,dictionary.get(key),dictionary.dfs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T00:08:14.162474Z",
     "start_time": "2019-03-27T00:08:14.157450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 在 1\n",
      "1 喜欢 1\n",
      "2 去 1\n",
      "3 还要 1\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "wordslist = [\"我在玉龙雪山\",\"我喜欢玉龙雪山\",\"我还要去玉龙雪山\"] \n",
    "# 切词\n",
    "textTest = [[word for word in jieba.cut(words)] for words in wordslist]\n",
    "# 生成字典\n",
    "dictionary = corpora.Dictionary(textTest,prune_at=2000000)\n",
    "dictionary.filter_n_most_frequent(2) #过滤出现次数最多的前两个词语\n",
    "for key in dictionary.keys():\n",
    "    print(key,dictionary.get(key),dictionary.dfs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T00:08:20.996972Z",
     "start_time": "2019-03-27T00:08:20.990988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 在 1\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "wordslist = [\"我在玉龙雪山\",\"我喜欢玉龙雪山\",\"我还要去玉龙雪山\"] \n",
    "# 切词\n",
    "textTest = [[word for word in jieba.cut(words)] for words in wordslist]\n",
    "# 生成字典\n",
    "dictionary = corpora.Dictionary(textTest,prune_at=2000000)\n",
    "dictionary.filter_tokens(good_ids=[0]) #good_ids=[0,2]表示仅保留编号为0,2的词语，bad_ids=[1,3]表示要删除编号为1,3的词语\n",
    "for key in dictionary.keys():\n",
    "    print(key,dictionary.get(key),dictionary.dfs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T00:08:32.545133Z",
     "start_time": "2019-03-27T00:08:32.539146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过滤后:\n",
      "0 我 3\n",
      "1 玉龙雪山 3\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "wordslist = [\"我在玉龙雪山\",\"我喜欢玉龙雪山\",\"我还要去玉龙雪山\"] \n",
    "# 切词\n",
    "textTest = [[word for word in jieba.cut(words)] for words in wordslist]\n",
    "# 生成字典\n",
    "dictionary = corpora.Dictionary(textTest,prune_at=2000000)\n",
    "# 如果想要过滤掉出现次数为1的词，可以使用以下代码\n",
    "ids=[]\n",
    "for key in dictionary.keys():\n",
    "    if dictionary.dfs[key]==1:\n",
    "        ids.append(key)\n",
    "dictionary.filter_tokens(bad_ids=ids)\n",
    "print(\"过滤后:\")\n",
    "for key in dictionary.keys():\n",
    "    print(key,dictionary.get(key),dictionary.dfs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T00:09:58.751818Z",
     "start_time": "2019-03-27T00:09:58.746848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '去', '玉龙雪山', '并且', '喜欢', '玉龙雪山']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(0, 1), (1, 2)], {'去': 1, '喜欢': 1, '并且': 1})\n"
     ]
    }
   ],
   "source": [
    "# 向量化\n",
    "# 将要向量化的数据,注意test是list<list>\n",
    "wordstest = \"我去玉龙雪山并且喜欢玉龙雪山\"\n",
    "# 切词\n",
    "test = [word for word in jieba.cut(wordstest)]\n",
    "test\n",
    "# 将数据向量化doc2bow(document, allow_update=False, return_missing=False)，其实这一步生成了向量化词袋\n",
    "corpus,missing = dictionary.doc2bow(test,return_missing=True)\n",
    "print((corpus,missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T01:07:58.406176Z",
     "start_time": "2019-03-27T01:07:58.380003Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['我', '去', '玉龙雪山', '并且', '喜欢', '玉龙雪山', '玉龙雪山'],\n",
       " ['我', '在', '天安门'],\n",
       " ['我', '在', '去往', '玉龙雪山', '的', '路上']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(0, '去'),\n",
       " (1, '喜欢'),\n",
       " (2, '并且'),\n",
       " (3, '我'),\n",
       " (4, '玉龙雪山'),\n",
       " (5, '在'),\n",
       " (6, '天安门'),\n",
       " (7, '去往'),\n",
       " (8, '的'),\n",
       " (9, '路上')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 3)],\n",
       " [(3, 1), (5, 1), (6, 1)],\n",
       " [(3, 1), (4, 1), (5, 1), (7, 1), (8, 1), (9, 1)]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(3, 1), (6, 1)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(6, 1.0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{0: 1, 1: 1, 2: 1, 3: 3, 4: 2, 5: 2, 6: 1, 7: 1, 8: 1, 9: 1}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{0: 1.5849625007211563,\n",
       " 1: 1.5849625007211563,\n",
       " 2: 1.5849625007211563,\n",
       " 3: 0.0,\n",
       " 4: 0.5849625007211562,\n",
       " 5: 0.5849625007211562,\n",
       " 6: 1.5849625007211563,\n",
       " 7: 1.5849625007211563,\n",
       " 8: 1.5849625007211563,\n",
       " 9: 1.5849625007211563}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<function gensim.matutils.unitvec(vec, norm='l2', return_norm=False)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 二.gensim的模型model模块，可以对corpus进行进一步的处理，比如tf-idf模型，lsi模型，lda模型等\n",
    "import jieba\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "wordstest_model = [\"我去玉龙雪山并且喜欢玉龙雪山玉龙雪山\",\"我在天安门\",\"我在去往玉龙雪山的路上\"]\n",
    "test_model = [[word for word in jieba.cut(words)] for words in wordstest_model]\n",
    "test_model\n",
    "dictionary = corpora.Dictionary(test_model) #先建立辞典\n",
    "list(dictionary.items())\n",
    "corpus_model= [dictionary.doc2bow(test) for test in test_model] #形成每句/文章的情况\n",
    "corpus_model\n",
    "\n",
    "# 目前只是生成了一个模型,并不是将对应的corpus转化后的结果,里面存储有各个单词的词频，文频等信息\n",
    "tfidf_model = models.TfidfModel(corpus_model)\n",
    "#使用测试文本来测试模型，提取关键词,test_bow提供当前文本词频，tfidf_model提供idf计算\n",
    "testword = \"我看天安门\"\n",
    "test_bow = dictionary.doc2bow([word for word in jieba.cut(testword)])\n",
    "test_bow\n",
    "tfidf_model[test_bow]\n",
    "\n",
    "tfidf_model.dfs #{单词id，在多少文档中出现}\n",
    "# # 通过gensim.models.tfidfmodel.df2idf(docfreq, totaldocs, log_base=2.0, add=0.0)方法计算idf值\n",
    "# # 即idf = add + log(totaldocs / doc_freq),这种算法可能会出现0值\n",
    "tfidf_model.idfs #{单词id，idf值}，\n",
    "tfidf_model.id2word #无返回值\n",
    "tfidf_model.num_docs #所有文章数目\n",
    "tfidf_model.normalize #是否规范化处理\n",
    "tfidf_model.num_nnz #每个文件中不重复词个数的和4+4 =8\n",
    "\n",
    "# # 将文档转化成tf-idf模式表示的向量\n",
    "# 保存模型和加载模型\n",
    "# tfidf_model.save('/tmp/foo.tfidf_model',)\n",
    "# tfidf_model = models.TfidfModel.load('/tmp/foo.tfidf_model',mmap=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T01:07:58.874073Z",
     "start_time": "2019-03-27T01:07:58.854065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.289*\"玉龙雪山\" + 0.131*\"去\" + 0.125*\"我\" + 0.125*\"并且\" + 0.112*\"喜欢\" + 0.045*\"在\" + 0.045*\"天安门\" + 0.043*\"路上\" + 0.042*\"去往\" + 0.042*\"的\"'),\n",
       " (1,\n",
       "  '0.226*\"玉龙雪山\" + 0.129*\"喜欢\" + 0.121*\"我\" + 0.111*\"并且\" + 0.101*\"去\" + 0.065*\"在\" + 0.064*\"天安门\" + 0.062*\"去往\" + 0.061*\"路上\" + 0.061*\"的\"'),\n",
       " (2,\n",
       "  '0.189*\"我\" + 0.186*\"在\" + 0.110*\"玉龙雪山\" + 0.109*\"的\" + 0.108*\"去往\" + 0.107*\"路上\" + 0.105*\"天安门\" + 0.029*\"去\" + 0.029*\"喜欢\" + 0.028*\"并且\"')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus_model, id2word=dictionary, num_topics=3)  # 通过LDA进行主题建模\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T01:07:59.693938Z",
     "start_time": "2019-03-27T01:07:59.686958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 1), (6, 1)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.16960569), (1, 0.17093119), (2, 0.65946317)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testword = \"我看天安门\"\n",
    "test_bow = dictionary.doc2bow([word for word in jieba.cut(testword)])\n",
    "test_bow\n",
    "lda[tfidf_model[test_bow]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T01:08:00.510792Z",
     "start_time": "2019-03-27T01:08:00.503738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 3)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.7592156), (1, 0.12749138), (2, 0.11329304)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testword = \"我去玉龙雪山并且喜欢玉龙雪山玉龙雪山\"\n",
    "test_bow = dictionary.doc2bow([word for word in jieba.cut(testword)])\n",
    "test_bow\n",
    "lda[tfidf_model[test_bow]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于多项式贝叶斯的增量学习的文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T00:38:45.605574Z",
     "start_time": "2019-03-29T00:38:45.594606Z"
    }
   },
   "outputs": [],
   "source": [
    "# 全角转半角\n",
    "def str_convert(content):\n",
    "    '''\n",
    "    将内容中的全角字符，包含英文字母、数字键、符号等转换为半角字符\n",
    "    :param content: 要转换的字符串内容\n",
    "    :return: 转换后的半角字符串\n",
    "    '''\n",
    "    new_str = ''  # 新字符串\n",
    "    for each_char in content:  # 循环读取每个字符\n",
    "        code_num = ord(each_char)  # 读取字符的ASCII值或Unicode值\n",
    "        if code_num == 12288:  # 全角空格直接转换\n",
    "            code_num = 32\n",
    "        elif (code_num >= 65281 and code_num <= 65374):  # 全角字符（除空格）根据关系转化\n",
    "            code_num -= 65248\n",
    "        new_str += chr(code_num)\n",
    "    return new_str\n",
    "\n",
    "# 解析文件内容\n",
    "def data_parse(data):\n",
    "    '''\n",
    "    从原始文件中解析出文本内容和标签数据\n",
    "    :param data: 包含代码的原始内容\n",
    "    :return: 以列表形式返回文本中的所有内容和对应标签\n",
    "    '''\n",
    "    raw_code = BeautifulSoup(data, \"lxml\")  # 建立BeautifulSoup对象\n",
    "    doc_code = raw_code.find_all('doc')  # 从包含文本的代码块中找到doc标签\n",
    "    content_list = []  # 建立空列表，用来存储每个content标签的内容\n",
    "    label_list = []  # 建立空列表，用来存储每个content对应的label的内容\n",
    "    for each_doc in doc_code:  # 循环读出每个doc标签\n",
    "        if len(each_doc) > 0:  # 如果dco标签的内容不为空\n",
    "            content_code = each_doc.find('content')  # 从包含文本的代码块中找到doc标签\n",
    "            raw_content = content_code.text  # 获取原始内容字符串\n",
    "            convert_content = str_convert(raw_content)  # 将全角转换为半角\n",
    "            content_list.append(convert_content)  # 将content文本内容加入列表\n",
    "\n",
    "            label_code = each_doc.find('url')  # 从包含文本的代码块中找到url标签\n",
    "            label_content = label_code.text  # 获取url信息\n",
    "            label = re.split('[/|.]', label_content)[2]  # 将URL做分割并提取子域名\n",
    "            label_list.append(label)  # 将子域名加入列表\n",
    "    return content_list, label_list\n",
    "\n",
    "\n",
    "# 交叉检验\n",
    "def cross_val(model_object, data, label):\n",
    "    '''\n",
    "    通过交叉检验计算每次增量学习后的模型得分\n",
    "    :param model_object: 每次增量学习后的模型对象\n",
    "    :param data: 训练数据集\n",
    "    :param label: 训练数据集对应的标签\n",
    "    :return: 交叉检验得分\n",
    "    '''\n",
    "    predict_label = model_object.predict(data)  # 预测测试集标签\n",
    "    score_tmp = round(accuracy_score(label, list(predict_label)), 4)  # 计算预测准确率\n",
    "    return score_tmp\n",
    "\n",
    "\n",
    "# word to vector\n",
    "def word_to_vector(data):\n",
    "    '''\n",
    "    将训练集文本数据转换为稀疏矩阵\n",
    "    :param data: 输入的文本列表\n",
    "    :return: 稀疏矩阵\n",
    "    '''\n",
    "    model_vector = HashingVectorizer(non_negative=True)  # 建立HashingVectorizer对象\n",
    "    vector_data = model_vector.fit_transform(data)  # 将输入文本转化为稀疏矩阵\n",
    "    return vector_data\n",
    "\n",
    "\n",
    "# label to vecotr\n",
    "def label_to_vector(label, unique_list):\n",
    "    '''\n",
    "    将文本标签转换为向量标签\n",
    "    :param label: 文本列表\n",
    "    :unique_list: 唯一值列表\n",
    "    :return: 向量标签列表\n",
    "    '''\n",
    "    for each_index, each_data in enumerate(label):  # 循环读取每个标签的索引及对应值\n",
    "        label[each_index] = unique_list.index(each_data)  # 将值替换为其索引\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T00:09:00.882314Z",
     "start_time": "2019-03-29T00:09:00.857356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<doc>\\n<url'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./datacode_for_book/chapter8/test_sets.txt','r',encoding='utf-8') as f:\n",
    "    test_data = f.read()\n",
    "test_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T00:18:10.312529Z",
     "start_time": "2019-03-29T00:18:09.239070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.反对宪法基本原则,危害国家安全、政权稳定统一的;煽动民族仇恨、民族歧视的;\\ue40c2.宣扬邪教和封建迷信的;散布谣言,破坏社会稳定的;侮辱诽谤他人,侵害他人合法权益的;\\ue40c3.散布淫秽、色情、赌博、暴力、凶杀、恐怖或者教唆犯罪的;'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'house'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "raw_code = BeautifulSoup(test_data, \"lxml\")  # 建立BeautifulSoup对象\n",
    "doc_code = raw_code.find_all('doc')  # 从包含文本的代码块中找到doc标签\n",
    "content_list = []  # 建立空列表，用来存储每个content标签的内容\n",
    "label_list = []  # 建立空列表，用来存储每个content对应的label的内容\n",
    "for each_doc in doc_code:  # 循环读出每个doc标签\n",
    "    if len(each_doc) > 0:  # 如果dco标签的内容不为空\n",
    "        content_code = each_doc.find('content')  # 从包含文本的代码块中找到doc标签\n",
    "        raw_content = content_code.text  # 获取原始内容字符串\n",
    "        convert_content = str_convert(raw_content)  # 将全角转换为半角\n",
    "        content_list.append(convert_content)  # 将content文本内容加入列表\n",
    "\n",
    "        label_code = each_doc.find('url')  # 从包含文本的代码块中找到url标签\n",
    "        label_content = label_code.text  # 获取url信息\n",
    "        label = re.split('[/|.]', label_content)[2]  # 将URL做分割并提取子域名\n",
    "        label_list.append(label)  # 将子域名加入列表\n",
    "content_list[0]\n",
    "label_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T00:18:11.010222Z",
     "start_time": "2019-03-29T00:18:10.877577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1864x1048576 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 119401 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer  # 文本转稀疏矩阵\n",
    "\n",
    "model_vector = HashingVectorizer(non_negative=True)  # 建立HashingVectorizer对象\n",
    "vector_data = model_vector.fit_transform(content_list)  # 将输入文本转化为稀疏矩阵\n",
    "vector_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T00:18:11.599091Z",
     "start_time": "2019-03-29T00:18:11.586124Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['house', 'sports', 'house']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1, 0, 1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list[:3]\n",
    "unique_list = ['sports', 'house', 'news']  # 标签唯一值列表\n",
    "for each_index, each_data in enumerate(label_list):  # 循环读取每个标签的索引及对应值\n",
    "    label_list[each_index] = unique_list.index(each_data)  # 将值替换为其索引\n",
    "label_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T00:55:05.106083Z",
     "start_time": "2019-03-29T00:55:05.076164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['焦点图\\ue40cAC米兰马可-波罗训练基地,身着红黑箭条衫挥汗奔跑的7号是谁?是一鸣惊人的帕托?还是舍瓦王者归来?是你!来自中国的你!足球朝圣2008,邀你与奥运同行,成为红黑军团的一员,体验顶级职业球员生活。11名天才足球少年将免费受训意大利,圆梦欧洲足球!\\ue40c全国范围内8-16岁有足球基础的少年均可报名参加5月和7月在京举行的选拔比赛,由资深教练和前国脚组成的评审团将最终选出11名足球朝圣者,接受专业培训并与当地青少年切磋技艺,参观圣西罗球场,亲临现场观看比赛。[活动简介]\\ue40c培训教练\\ue40cAC米兰的教练组将对我们选拔的每个学员进行初步评测,并定下相应的训练计划。\\ue40c负责训练评测的教练都是具有意大利足协培训认证的AC米兰著名教练,这其中包括球员时期鼎鼎有名的毛利齐奥-冈茨、菲里波-加利。在赛季期间,AC米兰俱乐部将安排学员们参观圣西罗球场。表现突出的球员还将有机会获邀前往米兰内洛基地,与众多球星亲密接触……[ 点击查看详细]\\ue40c毛利齐奥-冈茨:1990年代意甲著名前锋,曾效力于亚特兰大,国际米兰,佛罗伦萨,摩德纳和安科纳等俱乐部;曾于1997/98至1999/2000赛季效力AC Milan,并获得意甲冠军...[查看详细]\\ue40c菲里波-加利:1990年代AC Milan主力后卫,与荷兰三剑客共创56场联赛不败;为AC Milan效力超过12个赛季,5夺意甲冠军,3夺欧洲冠军杯...[查看详细]\\ue40c皮耶利诺-普拉蒂:1970年代AC Milan主力前锋,曾在1969年冠军杯决赛对阿贾克斯的比赛中上演帽子戏法;1967年意甲最佳射手;代表意大利国家队获得1968年欧锦赛冠军...[查看详细]\\ue40c训练基地\\ue40c青训球星\\ue40c米兰青年队拥有意大利最为出色的青训体系,以擅长培养领袖球员而出名,像巴雷西、马尔蒂尼,现役球员中,奥多、托尔多、科科、多纳代尔等球星,也曾是米兰青训体系培育出的球星。\\ue40c在这其中,巴雷西曾被视为世界上最好的自由人,米兰红黑的象征,他退役后,6号球衣也随之被米兰封存。巴雷西后,马尔蒂尼成为米兰当仁不让的灵魂,2007/08赛季将是这位伟大的队长效力红黑军团的最后一个赛季……[ 点击查看详细]\\ue40c保罗-马尔蒂尼,17岁起从青年队提拔至一队,1988年的欧洲锦标赛上崭露头角,曾在1994年被英国《世界足球》评为年度世界最佳球员,为米兰仅出战意甲联赛的次数就已超过600场...[查看详细]\\ue40c世界上最好的自由人,米兰红黑精神的象征,米兰六号的永远拥有者,在长达二十余年的职业生涯中只为米兰一家俱乐部效力,里维拉之后米兰伟大的队长与继承人... [查看详细]\\ue40c米兰、意大利国家队前队副,参加了1994、98年两届世界杯,1996、2000两届欧洲杯。90年代意大利最好的中场球员,被博班誉为意大利的大脑,遗憾的是未能在米兰结束职业生涯...[查看详细]']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['sports']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<1x1048576 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 102 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预测集\n",
    "with open('./datacode_for_book/chapter8/article.txt','r',encoding='utf-8') as f:\n",
    "    new_data = f.read()\n",
    "    \n",
    "new_raw_code = BeautifulSoup(new_data, \"lxml\")  # 建立BeautifulSoup对象\n",
    "new_doc_code = new_raw_code.find_all('doc')  # 从包含文本的代码块中找到doc标签\n",
    "new_content_list = []  # 建立空列表，用来存储每个content标签的内容\n",
    "new_label_list = []  # 建立空列表，用来存储每个content对应的label的内容\n",
    "for each_doc in new_doc_code:  # 循环读出每个doc标签\n",
    "    if len(each_doc) > 0:  # 如果dco标签的内容不为空\n",
    "        content_code = each_doc.find('content')  # 从包含文本的代码块中找到doc标签\n",
    "        raw_content = content_code.text  # 获取原始内容字符串\n",
    "        convert_content = str_convert(raw_content)  # 将全角转换为半角\n",
    "        new_content_list.append(convert_content)  # 将content文本内容加入列表\n",
    "\n",
    "        label_code = each_doc.find('url')  # 从包含文本的代码块中找到url标签\n",
    "        label_content = label_code.text  # 获取url信息\n",
    "        label = re.split('[/|.]', label_content)[2]  # 将URL做分割并提取子域名\n",
    "        new_label_list.append(label)  # 将子域名加入列表\n",
    "new_content_list\n",
    "new_label_list\n",
    "\n",
    "model_vector = HashingVectorizer(non_negative=True)  # 建立HashingVectorizer对象\n",
    "new_vector_data = model_vector.fit_transform(new_content_list)  # 将输入文本转化为稀疏矩阵\n",
    "new_vector_data\n",
    "\n",
    "unique_list = ['sports', 'house', 'news']  # 标签唯一值列表\n",
    "for each_index, each_data in enumerate(new_label_list):  # 循环读取每个标签的索引及对应值\n",
    "    new_label_list[each_index] = unique_list.index(each_data)  # 将值替换为其索引\n",
    "new_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T00:55:20.526495Z",
     "start_time": "2019-03-29T00:55:05.901262Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************incremental learning...*******************\n",
      "training file: news.sohunews.010806.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training file: news.sohunews.020806.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training file: news.sohunews.030806.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training file: news.sohunews.040806.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training file: news.sohunews.050806.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training file: news.sohunews.060806.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training file: news.sohunews.070806.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training file: news.sohunews.080806.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training file: news.sohunews.110806.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training file: news.sohunews.120806.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************cross validation score:*******************\n",
      "[0.8707, 0.9013, 0.9067, 0.9088, 0.9099, 0.912, 0.9147, 0.9142, 0.9147, 0.9158]\n",
      "*********************predicted labels:**********************\n",
      "[[0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
      "************************true labels:************************\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# 增量学习\n",
    "import os\n",
    "from sklearn.naive_bayes import MultinomialNB  # 贝叶斯分类器\n",
    "from sklearn.metrics import accuracy_score  # 分类评估指标\n",
    "score_list = []\n",
    "pre_list = []\n",
    "model_nb = MultinomialNB()  # 建立MultinomialNB模型对象\n",
    "\n",
    "print ('{:*^60}'.format('incremental learning...'))\n",
    "for root, dirs, files in os.walk('./datacode_for_book/chapter8/news_data'):  # 分别读取遍历目录下的根目录、子目录和文件列表\n",
    "    for file in files:  # 读取每个文件\n",
    "        file_name = os.path.join(root, file)  # 将目录路径与文件名合并为带有完整路径的文件名\n",
    "        print ('training file: %s' % file)\n",
    "        # 增量训练\n",
    "        with open(file_name,'r',encoding='utf-8') as f:  # 以只读方式打开文件\n",
    "            data = f.read()  # 读取文件内容\n",
    "        content, label = data_parse(data)  # 解析文本内容和标签\n",
    "        data_vector = word_to_vector(content)  # 将文本内容向量化\n",
    "        label_vecotr = label_to_vector(label, unique_list)  # 将标签内容向量化\n",
    "        model_nb.partial_fit(data_vector, label_vecotr, classes=np.array([0, 1, 2]))  # 增量学习\n",
    "        # 交叉检验\n",
    "        score_list.append(cross_val(model_nb, vector_data, label_list))  # 将交叉检验结果存入列表\n",
    "        # 增量预测\n",
    "        predict_y = model_nb.predict(new_vector_data)  # 预测内容标签\n",
    "        pre_list.append(predict_y.tolist())\n",
    "\n",
    "print ('{:*^60}'.format('cross validation score:'))\n",
    "print (score_list)  # 打印输出每次交叉检验得分\n",
    "print ('{:*^60}'.format('predicted labels:'))\n",
    "print (pre_list)  # 打印输出每次预测标签索引值\n",
    "print ('{:*^60}'.format('true labels:'))\n",
    "print (new_label_list)  # 打印输出正确的标签值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
